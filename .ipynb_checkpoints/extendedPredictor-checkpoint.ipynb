{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "import operator\n",
    "import preprocessor as pre\n",
    "import regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Turkish Fasttext model\n",
    "fasttextModel = KeyedVectors.load_word2vec_format('data/fasttext/wiki.tr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data into a data-frame\n",
    "# There should be all the account for four different categories\n",
    "# I removed 10 accounts before putting the directory into Github\n",
    "# I left 2 sample account in the directory data/trainAccount\n",
    "retrievedAccounts = [\"iuefsosyoloji\",\"sosyolojidivani\",\"sosyolojibolumu\", \"sosyo_kitap\",\n",
    "                     \"setadc\",\"tc_disisleri\",\"yeniturkiye\",\n",
    "                     \"iksv_istanbul\",\"issanat\", \"ntvksanat\",\n",
    "                     \"ihhinsaniyardim\",\"diyanetvakfi\",\"turkkizilayi\"]\n",
    "\n",
    "dataFrameList = []\n",
    "for anAccount in retrievedAccounts:\n",
    "    dataframe = pd.read_csv(\"data/trainAccount/\" + anAccount + '_cleaned.csv', sep=',', header=None, names=[\"label\",\"tweets\"])\n",
    "    dataFrameList.append(dataframe)\n",
    "gatheredDataFrame= pd.concat(dataFrameList).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Seperate tweets and labels into to data-frames\n",
    "tweets = gatheredDataFrame['tweets'].values\n",
    "labels = gatheredDataFrame['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert labels to binary representations\n",
    "labelMapper = {\"politics\":[1,0,0,0], \"art\":[0,1,0,0], \"society\":[0,0,1,0], \"charity\":[0,0,0,1]}\n",
    "labelsConverted = []\n",
    "for label in labels:\n",
    "    labelsConverted.append(labelMapper[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to avarage all word-vectors (calculating with provided model) within a sentence\n",
    "def bagOfWords(sentences, model, vectorLength):\n",
    "    returnMatrix = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        wordCount = len(words)\n",
    "        temporaryList = np.zeros(vectorLength)\n",
    "        for word in words:\n",
    "            try:\n",
    "                temporaryList += model[word]\n",
    "            except:\n",
    "                wordCount -= 1\n",
    "        if wordCount == 0:\n",
    "            avarageList = np.zeros(vectorLength)\n",
    "        else:\n",
    "            avarageList = temporaryList/wordCount\n",
    "        returnMatrix.append(avarageList)\n",
    "    return returnMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorLength = 300\n",
    "# Convert tweets into bag-of-words representation\n",
    "bagOfWordsMatrix = bagOfWords(tweets, fasttextModel, vectorLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data as train and test with the ratio 4/5\n",
    "sperator = int(len(bagOfWordsMatrix)*4/5)\n",
    "trainFeature = np.array(bagOfWordsMatrix[:sperator])\n",
    "trainLabel = np.array(labelsConverted[:sperator])\n",
    "testFeature = np.array(bagOfWordsMatrix[sperator:])\n",
    "testLabel = np.array(labelsConverted[sperator:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainCount = sperator\n",
    "testCount = len(bagOfWordsMatrix) - len(trainFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set Parameters for neural network\n",
    "learningRate = 0.001\n",
    "trainingEpochs = 150\n",
    "batchSize = 128\n",
    "displayStep = 1\n",
    "inputLayer = vectorLength\n",
    "firstHidden = 2 ** 6\n",
    "secondHidden = 2 ** 7\n",
    "outputLayer = 2 ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow arrangements\n",
    "x = tf.placeholder(\"float\", [None, inputLayer])\n",
    "y = tf.placeholder(\"float\", [None, outputLayer])\n",
    "\n",
    "# Function of a neural network model\n",
    "def multilayerPerceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    firstLayer = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    firstLayer = tf.nn.relu(firstLayer)\n",
    "    # Hidden layer with RELU activation\n",
    "    secondLayer = tf.add(tf.matmul(firstLayer, weights['h2']), biases['b2'])\n",
    "    secondLayer = tf.nn.relu(secondLayer)\n",
    "    # Output layer with linear activation\n",
    "    outputLayer = tf.matmul(secondLayer, weights['out']) + biases['out']\n",
    "    return outputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store weight and biases of each layer\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([inputLayer, firstHidden])),\n",
    "    'h2': tf.Variable(tf.random_normal([firstHidden, secondHidden])),\n",
    "    'out': tf.Variable(tf.random_normal([secondHidden, outputLayer]))}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([firstHidden])),\n",
    "    'b2': tf.Variable(tf.random_normal([secondHidden])),\n",
    "    'out': tf.Variable(tf.random_normal([outputLayer]))}\n",
    "\n",
    "# Construct the model\n",
    "prediction = multilayerPerceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learningRate).minimize(cost)\n",
    "\n",
    "# Initializing the Variables\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "# Calculate accuracy\n",
    "correctPredictions = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPredictions, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 48.537809552\n",
      "Epoch: 0002 cost= 16.171645204\n",
      "Epoch: 0003 cost= 11.212101183\n",
      "Epoch: 0004 cost= 8.639849621\n",
      "Epoch: 0005 cost= 6.996074763\n",
      "Epoch: 0006 cost= 5.833510590\n",
      "Epoch: 0007 cost= 4.943300272\n",
      "Epoch: 0008 cost= 4.242936794\n",
      "Epoch: 0009 cost= 3.666098441\n",
      "Epoch: 0010 cost= 3.186154503\n",
      "Epoch: 0011 cost= 2.782363186\n",
      "Epoch: 0012 cost= 2.451660262\n",
      "Epoch: 0013 cost= 2.174097709\n",
      "Epoch: 0014 cost= 1.936227339\n",
      "Epoch: 0015 cost= 1.733503498\n",
      "Epoch: 0016 cost= 1.564142109\n",
      "Epoch: 0017 cost= 1.419505253\n",
      "Epoch: 0018 cost= 1.297071832\n",
      "Epoch: 0019 cost= 1.193733717\n",
      "Epoch: 0020 cost= 1.104215101\n",
      "Epoch: 0021 cost= 1.025567705\n",
      "Epoch: 0022 cost= 0.957372121\n",
      "Epoch: 0023 cost= 0.898226658\n",
      "Epoch: 0024 cost= 0.846043421\n",
      "Epoch: 0025 cost= 0.800914506\n",
      "Epoch: 0026 cost= 0.762501714\n",
      "Epoch: 0027 cost= 0.727833680\n",
      "Epoch: 0028 cost= 0.697354016\n",
      "Epoch: 0029 cost= 0.669916745\n",
      "Epoch: 0030 cost= 0.645100099\n",
      "Epoch: 0031 cost= 0.622358245\n",
      "Epoch: 0032 cost= 0.601191793\n",
      "Epoch: 0033 cost= 0.582101635\n",
      "Epoch: 0034 cost= 0.564335163\n",
      "Epoch: 0035 cost= 0.548343104\n",
      "Epoch: 0036 cost= 0.533598155\n",
      "Epoch: 0037 cost= 0.520240645\n",
      "Epoch: 0038 cost= 0.507402442\n",
      "Epoch: 0039 cost= 0.495031719\n",
      "Epoch: 0040 cost= 0.484140683\n",
      "Epoch: 0041 cost= 0.473554671\n",
      "Epoch: 0042 cost= 0.463974129\n",
      "Epoch: 0043 cost= 0.454873827\n",
      "Epoch: 0044 cost= 0.445319951\n",
      "Epoch: 0045 cost= 0.437106068\n",
      "Epoch: 0046 cost= 0.428066114\n",
      "Epoch: 0047 cost= 0.420184379\n",
      "Epoch: 0048 cost= 0.412340839\n",
      "Epoch: 0049 cost= 0.405272589\n",
      "Epoch: 0050 cost= 0.398865090\n",
      "Epoch: 0051 cost= 0.392054392\n",
      "Epoch: 0052 cost= 0.386670597\n",
      "Epoch: 0053 cost= 0.380033311\n",
      "Epoch: 0054 cost= 0.374095683\n",
      "Epoch: 0055 cost= 0.368747430\n",
      "Epoch: 0056 cost= 0.363097419\n",
      "Epoch: 0057 cost= 0.358065024\n",
      "Epoch: 0058 cost= 0.353496664\n",
      "Epoch: 0059 cost= 0.349986335\n",
      "Epoch: 0060 cost= 0.344517657\n",
      "Epoch: 0061 cost= 0.339774467\n",
      "Epoch: 0062 cost= 0.335529075\n",
      "Epoch: 0063 cost= 0.331641667\n",
      "Epoch: 0064 cost= 0.327329786\n",
      "Epoch: 0065 cost= 0.323813595\n",
      "Epoch: 0066 cost= 0.319344529\n",
      "Epoch: 0067 cost= 0.316711357\n",
      "Epoch: 0068 cost= 0.312956907\n",
      "Epoch: 0069 cost= 0.309182523\n",
      "Epoch: 0070 cost= 0.305605488\n",
      "Epoch: 0071 cost= 0.302830777\n",
      "Epoch: 0072 cost= 0.299486665\n",
      "Epoch: 0073 cost= 0.296470004\n",
      "Epoch: 0074 cost= 0.293058448\n",
      "Epoch: 0075 cost= 0.288655520\n",
      "Epoch: 0076 cost= 0.286192508\n",
      "Epoch: 0077 cost= 0.284321249\n",
      "Epoch: 0078 cost= 0.282244015\n",
      "Epoch: 0079 cost= 0.278127778\n",
      "Epoch: 0080 cost= 0.274612541\n",
      "Epoch: 0081 cost= 0.272797192\n",
      "Epoch: 0082 cost= 0.269503327\n",
      "Epoch: 0083 cost= 0.267345484\n",
      "Epoch: 0084 cost= 0.264651568\n",
      "Epoch: 0085 cost= 0.262842843\n",
      "Epoch: 0086 cost= 0.260065349\n",
      "Epoch: 0087 cost= 0.259170490\n",
      "Epoch: 0088 cost= 0.257325005\n",
      "Epoch: 0089 cost= 0.256730354\n",
      "Epoch: 0090 cost= 0.254601769\n",
      "Epoch: 0091 cost= 0.254188598\n",
      "Epoch: 0092 cost= 0.253069984\n",
      "Epoch: 0093 cost= 0.253559233\n",
      "Epoch: 0094 cost= 0.251584111\n",
      "Epoch: 0095 cost= 0.249900596\n",
      "Epoch: 0096 cost= 0.245753327\n",
      "Epoch: 0097 cost= 0.241525234\n",
      "Epoch: 0098 cost= 0.236034001\n",
      "Epoch: 0099 cost= 0.232620622\n",
      "Epoch: 0100 cost= 0.228117536\n",
      "Epoch: 0101 cost= 0.226352477\n",
      "Epoch: 0102 cost= 0.221575995\n",
      "Epoch: 0103 cost= 0.220029490\n",
      "Epoch: 0104 cost= 0.216074908\n",
      "Epoch: 0105 cost= 0.214444566\n",
      "Epoch: 0106 cost= 0.211059989\n",
      "Epoch: 0107 cost= 0.209038914\n",
      "Epoch: 0108 cost= 0.206626412\n",
      "Epoch: 0109 cost= 0.204452969\n",
      "Epoch: 0110 cost= 0.201769394\n",
      "Epoch: 0111 cost= 0.200133098\n",
      "Epoch: 0112 cost= 0.197894784\n",
      "Epoch: 0113 cost= 0.196535738\n",
      "Epoch: 0114 cost= 0.194790825\n",
      "Epoch: 0115 cost= 0.193003999\n",
      "Epoch: 0116 cost= 0.191661689\n",
      "Epoch: 0117 cost= 0.190073078\n",
      "Epoch: 0118 cost= 0.187710596\n",
      "Epoch: 0119 cost= 0.186670928\n",
      "Epoch: 0120 cost= 0.184477220\n",
      "Epoch: 0121 cost= 0.183505418\n",
      "Epoch: 0122 cost= 0.181597378\n",
      "Epoch: 0123 cost= 0.180818176\n",
      "Epoch: 0124 cost= 0.178742422\n",
      "Epoch: 0125 cost= 0.178073868\n",
      "Epoch: 0126 cost= 0.176260286\n",
      "Epoch: 0127 cost= 0.175679528\n",
      "Epoch: 0128 cost= 0.174358444\n",
      "Epoch: 0129 cost= 0.173731929\n",
      "Epoch: 0130 cost= 0.171212047\n",
      "Epoch: 0131 cost= 0.168661379\n",
      "Epoch: 0132 cost= 0.168285988\n",
      "Epoch: 0133 cost= 0.166174973\n",
      "Epoch: 0134 cost= 0.164722393\n",
      "Epoch: 0135 cost= 0.163488067\n",
      "Epoch: 0136 cost= 0.161724635\n",
      "Epoch: 0137 cost= 0.160888119\n",
      "Epoch: 0138 cost= 0.159427477\n",
      "Epoch: 0139 cost= 0.158190784\n",
      "Epoch: 0140 cost= 0.156100502\n",
      "Epoch: 0141 cost= 0.154966679\n",
      "Epoch: 0142 cost= 0.153372291\n",
      "Epoch: 0143 cost= 0.152495083\n",
      "Epoch: 0144 cost= 0.151185266\n",
      "Epoch: 0145 cost= 0.149989745\n",
      "Epoch: 0146 cost= 0.149915197\n",
      "Epoch: 0147 cost= 0.147758023\n",
      "Epoch: 0148 cost= 0.146072890\n",
      "Epoch: 0149 cost= 0.144675038\n",
      "Epoch: 0150 cost= 0.142729498\n",
      "Optimization Finished!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(initializer)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(trainingEpochs):\n",
    "    averageCost = 0.\n",
    "    totalBatch = int(trainCount/batchSize)\n",
    "\n",
    "    # Loop over all batches\n",
    "    batchCounter = 0\n",
    "    for i in range(totalBatch):\n",
    "        batchFeature, batchLabel = trainFeature[batchCounter:batchCounter + batchSize],\\\n",
    "                                   trainLabel[batchCounter:batchCounter + batchSize]\n",
    "        # Run optimization and cost operations\n",
    "        _, c = session.run([optimizer, cost], feed_dict={x: batchFeature, y: batchLabel})\n",
    "\n",
    "        # Calculate average cost\n",
    "        averageCost += c / totalBatch\n",
    "        batchCounter = batchCounter + batchSize\n",
    "\n",
    "    if epoch % displayStep == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(averageCost))\n",
    "\n",
    "print(\"Optimization Finished!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate Accuracy \n",
    "correctPredictions = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "accuracyTensor = tf.reduce_mean(tf.cast(correctPredictions, \"float\"))\n",
    "probabilityLabels = tf.argmax(prediction, 1)\n",
    "trainAccuracy, _ = session.run([accuracyTensor, probabilityLabels], feed_dict={x:trainFeature, y:trainLabel})\n",
    "testAccuracy, _ = session.run([accuracyTensor, probabilityLabels], feed_dict={x:testFeature, y:testLabel})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.956633\n",
      "Test Accuracy : 0.803368\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy : %f\" % trainAccuracy)\n",
    "print(\"Test Accuracy : %f\" % testAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interestFinder(userName):\n",
    "  \n",
    "    # Load user tweets and assign a label to each of them\n",
    "    currentDataframe = pd.read_csv(\"data/clean/\" + userName + '_cleaned.csv', sep=',', header=None, names=[\"label\",\"tweets\"])\n",
    "    currentTweets = currentDataframe['tweets'].values\n",
    "    currentBagOfWordsMatrix = bagOfWords(currentTweets, fasttextModel, 300)\n",
    "    prediction = multilayerPerceptron(x, weights, biases)\n",
    "    probabilityLabels = tf.argmax(prediction, 1)\n",
    "    predictedLabels = session.run(probabilityLabels, feed_dict={x:currentBagOfWordsMatrix})\n",
    "    \n",
    "    # Calculate frequency of labels\n",
    "    labelCount = {\"politics\":0, \"art\":0, \"society\":0, \"charity\":0}\n",
    "    for prediction in predictedLabels:\n",
    "        if prediction == 0:\n",
    "            labelCount['politics'] += 1\n",
    "        elif prediction == 1:\n",
    "            labelCount['art'] += 1\n",
    "        elif prediction == 2:\n",
    "            labelCount['society'] += 1\n",
    "        elif prediction == 3:\n",
    "            labelCount['charity'] += 1\n",
    "    \n",
    "    total = float(len(currentDataframe))\n",
    "    labelFrequency = {key: value / total for key, value in labelCount.iteritems()}\n",
    "    return labelFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open input file to retrieve users from and a file to write topic distribution of each user\n",
    "inputFile = open('data/graph/node.csv', 'rt')\n",
    "outputFile = open('data/graph/nodeWithScore.csv', 'wt')\n",
    "reader = csv.reader(inputFile)\n",
    "writer = csv.writer(outputFile, delimiter=',', lineterminator='\\n')\n",
    "writer.writerow(['', 'id', 'label', 'politics', 'art', 'society', 'charity', 'interest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in reader:\n",
    "    userName = line[2]\n",
    "    interestDictionary = {\"politics\":0, \"art\":0, \"society\":0, \"charity\":0}\n",
    "    mainInterest = \"n/a\"\n",
    "    \n",
    "    if userName != \"n/a\":\n",
    "        try: \n",
    "            # Load user tweets and assign a label to each of them\n",
    "            currentDataframe = pd.read_csv(\"data/clean/\" + userName + '_cleaned.csv', sep=',', header=None, names=[\"label\",\"tweets\"])\n",
    "            currentTweets = currentDataframe['tweets'].values\n",
    "            currentBagOfWordsMatrix = bagOfWords(currentTweets, fasttextModel, 300)\n",
    "            predictedLabels = session.run(probabilityLabels, feed_dict={x:currentBagOfWordsMatrix})\n",
    "\n",
    "            # Calculate frequency of labels\n",
    "            labelCount = {\"politics\":0, \"art\":0, \"society\":0, \"charity\":0}\n",
    "            for prediction in predictedLabels:\n",
    "                if prediction == 0:\n",
    "                    labelCount['politics'] += 1\n",
    "                elif prediction == 1:\n",
    "                    labelCount['art'] += 1\n",
    "                elif prediction == 2:\n",
    "                    labelCount['society'] += 1\n",
    "                elif prediction == 3:\n",
    "                    labelCount['charity'] += 1\n",
    "\n",
    "            total = float(len(currentDataframe))\n",
    "            interestDictionary = {key: value / total for key, value in labelCount.items()}\n",
    "            mainInterest = max(interestDictionary.items(), key=operator.itemgetter(1))[0]\n",
    "            print(\"Success : %s\" % userName)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"Error : %s for %s\" % (error, userName))\n",
    "\n",
    "    writer.writerow([line[0], line[1], line[2], interestDictionary['politics'], \n",
    "                     interestDictionary['art'], interestDictionary['society'],\n",
    "                     interestDictionary['charity'], mainInterest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Since the actual output is too long, I just included a sample\n",
    "User : philobuny\n",
    "User : sabankardas\n",
    "User : ahmet_ors\n",
    "User : htcugurist\n",
    "User : Marmaraaa\n",
    "User : Donoughtella\n",
    "User : usamedegirmenci\n",
    "Error : File b'data/clean/usamedegirmenci_cleaned.csv' does not exist\n",
    "User : SehirClubs\n",
    "User : ozhanneslihan\n",
    "Error : File b'data/clean/ozhanneslihan_cleaned.csv' does not exist\n",
    "User : bvuslatcelik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFile.close()\n",
    "outputFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing texts to train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFile = open('data/graph/node.csv', 'rt')\n",
    "reader = csv.reader(inputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "politicsList = list()\n",
    "artList = list()\n",
    "dailyList = list()\n",
    "charityList = list()\n",
    "\n",
    "for line in reader:\n",
    "    userName = line[2]\n",
    "    \n",
    "    if userName != \"n/a\":\n",
    "        try: \n",
    "            # Load user tweets and assign a label to each of them\n",
    "            currentDataframe = pd.read_csv(\"data/clean/\" + userName + '_cleaned.csv', sep=',', header=None, names=[\"label\",\"tweets\"])\n",
    "            currentTweets = currentDataframe['tweets'].values\n",
    "            currentBagOfWordsMatrix = bagOfWords(currentTweets, fasttextModel, 300)\n",
    "            predictedLabels = session.run(probabilityLabels, feed_dict={x:currentBagOfWordsMatrix})\n",
    "\n",
    "            # Calculate frequency of labels\n",
    "            labelCount = {\"politics\":0, \"art\":0, \"society\":0, \"charity\":0}\n",
    "            for prediction in predictedLabels:\n",
    "                if prediction == 0:\n",
    "                    labelCount['politics'] += 1\n",
    "                elif prediction == 1:\n",
    "                    labelCount['art'] += 1\n",
    "                elif prediction == 2:\n",
    "                    labelCount['society'] += 1\n",
    "                elif prediction == 3:\n",
    "                    labelCount['charity'] += 1\n",
    "\n",
    "            total = float(len(currentDataframe))\n",
    "            interestDictionary = {key: value / total for key, value in labelCount.items()}\n",
    "            mainInterest = max(interestDictionary.items(), key=operator.itemgetter(1))[0]\n",
    "            \n",
    "            if mainInterest == \"politics\":\n",
    "                politicsList.append(int(line[1]))\n",
    "            elif mainInterest == \"art\":\n",
    "                artList.append(int(line[1]))\n",
    "            elif mainInterest == \"society\":\n",
    "                dailyList.append(int(line[1]))\n",
    "            elif mainInterest == \"charity\":\n",
    "                charityList.append(int(line[1]))  \n",
    "            print(\"Success : %s\" % userName)\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"Error : %s for %s\" % (error, userName))\n",
    "        \n",
    "politicsList = set(politicssList)\n",
    "artList = set(arttList)\n",
    "dailyList = set(dailyList)\n",
    "charityList = set(charityList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Since the actual output is too long, I just included a sample\n",
    "Success : htcugurist\n",
    "Success : Marmaraaa\n",
    "Success : Donoughtella\n",
    "Error : File b'data/clean/usamedegirmenci_cleaned.csv' does not exist for usamedegirmenci\n",
    "Success : SehirClubs\n",
    "Success : bvuslatcelik\n",
    "Success : zepaltinbas\n",
    "Success : mhmmtmz\n",
    "Success : ssuheyl\n",
    "Error : File b'data/clean/zeynepkoyuncu__cleaned.csv' does not exist for zeynepkoyuncu_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved userTweetDictionary\n",
    "userTweetDictionary = np.load('data/dictionary/sehirTweets.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeTweets(categoryList, dataName):\n",
    "    # Clean Tweets\n",
    "    cleanedTweetDictionary = dict()\n",
    "    pre.set_options(pre.OPT.URL, pre.OPT.EMOJI, pre.OPT.SMILEY, pre.OPT.MENTION)\n",
    "    for userID in userTweetDictionary:\n",
    "        if userID in categoryList:\n",
    "            tweetsList = list()\n",
    "            for tweet in userTweetDictionary[userID]:\n",
    "                if tweet[5] == 'tr':\n",
    "                    tweetsList += [pre.clean(tweet[0])]\n",
    "            cleanedTweetDictionary[userID] = tweetsList\n",
    "\n",
    "    originalString = str()\n",
    "    for userID in cleanedTweetDictionary:\n",
    "        for tweet in cleanedTweetDictionary[userID]:\n",
    "            currentString = str()\n",
    "            for word in tweet.split():\n",
    "                word = regex.sub(u'[^\\p{Latin}]', u'', word)\n",
    "                currentString += word\n",
    "                currentString += \" \"\n",
    "            originalString += currentString.strip(\"RT \").lower() + \".\\n\"\n",
    "    \n",
    "    textFile = open(\"data/text/\" + dataName + \"/input.txt\", \"wt\")\n",
    "    textFile.write(originalString)\n",
    "    textFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writeTweets(politicsList, \"politics\")\n",
    "writeTweets(artList, \"art\")\n",
    "writeTweets(dailyList, \"daily\")\n",
    "writeTweets(charityList, \"charity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
